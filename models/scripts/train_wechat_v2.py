# -*- coding: utf-8 -*-
"""
å¾®ä¿¡èŠå¤©æ„å›¾è¯†åˆ«æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰

ç‰¹ç‚¹ï¼š
1. åŸºäº telemarketing_intent_cn æ•°æ®é›†
2. é»‘åå•è¿‡æ»¤ä¸é€‚åˆå¾®ä¿¡èŠå¤©çš„æ„å›¾
3. æ”¯æŒæ··åˆ crosswoz æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
4. æ ·æœ¬å¹³è¡¡å’Œæ•°æ®å¢å¼º
5. é€‚é… Windows æœ¬åœ°è®­ç»ƒ

ç›´æ¥è¿è¡Œï¼špython train_wechat_v2.py
"""

import json
import os
from collections import Counter
from pathlib import Path

import torch
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.model_selection import train_test_split
import numpy as np


# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================

# è‡ªåŠ¨æ£€æµ‹é¡¹ç›®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent.absolute()
if SCRIPT_DIR.name == "scripts":
    PROJECT_ROOT = SCRIPT_DIR.parent
else:
    PROJECT_ROOT = SCRIPT_DIR

# æ•°æ®é›†è·¯å¾„
DATA_DIR = PROJECT_ROOT / "few_shot_intent_sft" / "data"
TELEMARKETING_DATA = DATA_DIR / "telemarketing_intent_cn.jsonl"
CROSSWOZ_DATA = DATA_DIR / "crosswoz.jsonl"  # å¯é€‰

# æ¨¡å‹ä¿å­˜è·¯å¾„
OUTPUT_DIR = PROJECT_ROOT / "wechat_intent_model"

# è®­ç»ƒå‚æ•°
MIN_SAMPLES = 20  # æ¯ä¸ªæ„å›¾æœ€å°‘æ ·æœ¬æ•°ï¼ˆæé«˜è´¨é‡ï¼‰
MAX_SAMPLES_PER_INTENT = 300  # æ¯ä¸ªæ„å›¾æœ€å¤šæ ·æœ¬æ•°ï¼ˆå¹³è¡¡æ•°æ®ï¼‰
BATCH_SIZE = 16
EPOCHS = 4  # å¢åŠ åˆ°4è½®
TEST_SIZE = 0.2
RANDOM_SEED = 42

# æ˜¯å¦æ··åˆ CrossWOZ æ•°æ®é›†ï¼ˆæ›´è‡ªç„¶çš„å¯¹è¯ï¼‰
USE_CROSSWOZ = True


# ============================================================================
# å¾®ä¿¡èŠå¤©æ„å›¾é»‘åå•
# ============================================================================

# ä¸é€‚åˆå¾®ä¿¡èŠå¤©çš„æ„å›¾ï¼ˆç”µé”€ç‰¹å®šã€æ•æ„Ÿå†…å®¹ç­‰ï¼‰
BLACKLIST_INTENTS = {
    # ç”µé”€ç‰¹å®šæ„å›¾
    "æŸ¥è¯¢ç±»",
    "æŸ¥è¯¢(äº§å“ä¿¡æ¯)",
    "æŸ¥è¯¢(ä»·æ ¼)",
    "æŸ¥è¯¢(ä¼˜æƒ )",
    "æŸ¥è¯¢(åº“å­˜)",
    "æŸ¥è¯¢(ç‰©æµ)",
    "æŸ¥è¯¢(è®¢å•)",
    "æŸ¥è¯¢(è´¦æˆ·)",
    "æŸ¥è¯¢(ä½™é¢)",
    "å®ä½“(äº§å“)",
    "å®ä½“(ä»·æ ¼)",
    "å®ä½“(æ—¶é—´)",
    "å®ä½“(åœ°ç‚¹)",
    "å®ä½“(äººå)",
    "å®ä½“(å…¬å¸)",
    "å®ä½“è¯†åˆ«",
    "äº§å“æ¨è",
    "ä¿ƒé”€æ´»åŠ¨",
    "ä¼˜æƒ ä¿¡æ¯",
    "ä¸‹å•",
    "æ”¯ä»˜",
    "é€€æ¬¾",
    "æŠ•è¯‰",
    "å”®å",
    # æ•æ„Ÿå†…å®¹
    "æ”¿æ²»æ•æ„Ÿ",
    "æ±¡è¨€ç§½è¯­",
    "è‰²æƒ…ä½ä¿—",
    "æš´åŠ›è¡€è…¥",
    "è¿æ³•çŠ¯ç½ª",
    "å¹¿å‘Šè¥é”€",
    "è¯ˆéª—ä¿¡æ¯",
    # ä¸å¸¸ç”¨æˆ–å®¹æ˜“è¯¯åˆ¤çš„æ„å›¾
    "è‚¯å®š(æ²¡é—®é¢˜)",  # å®¹æ˜“è¯¯åˆ¤"ä½ è„‘å­æ²¡é—®é¢˜å§"
    "å¦å®š(æ²¡æœ‰)",  # å®¹æ˜“è¯¯åˆ¤
    "è½¬äººå·¥",
    "æŒ‚æ–­ç”µè¯",
    "ä¿æŒé€šè¯",
    "é‡å¤",
    "æ¾„æ¸…",
    "ç¡®è®¤ä¿¡æ¯",
    "æ ¸å®èº«ä»½",
    "å½•éŸ³æç¤º",
    "ç³»ç»Ÿæç¤º",
}

# ä¿ç•™çš„é€šç”¨å¯¹è¯æ„å›¾
KEEP_INTENTS = {
    # é—®å€™å’Œç¤¼è²Œ
    "æ‹›å‘¼ç”¨è¯­",
    "ç¤¼è²Œç”¨è¯­",
    "æ„Ÿè°¢",
    "é“æ­‰",
    "é—®å€™",
    "å‘Šåˆ«",
    "ç»“æŸç”¨è¯­",
    # è‚¯å®šå’Œå¦å®š
    "è‚¯å®š",
    "è‚¯å®š(å¥½çš„)",
    "è‚¯å®š(æ˜¯çš„)",
    "è‚¯å®š(å¯ä»¥)",
    "è‚¯å®š(åŒæ„)",
    "å¦å®š",
    "å¦å®š(ä¸æ˜¯)",
    "å¦å®š(ä¸è¦)",
    "å¦å®š(ä¸å¯ä»¥)",
    "æ‹’ç»",
    # ç–‘é—®
    "ç–‘é—®",
    "ç–‘é—®(æ˜¯ä»€ä¹ˆ)",
    "ç–‘é—®(ä¸ºä»€ä¹ˆ)",
    "ç–‘é—®(æ€ä¹ˆæ ·)",
    "ç–‘é—®(åœ¨å“ªé‡Œ)",
    "ç–‘é—®(ä»€ä¹ˆæ—¶å€™)",
    "åé—®",
    # æƒ…æ„Ÿ
    "å¼€å¿ƒ",
    "éš¾è¿‡",
    "ç”Ÿæ°”",
    "æƒŠè®¶",
    "æ‹…å¿ƒ",
    "æ— èŠ",
    "å…´å¥‹",
    # è¯·æ±‚å’Œå»ºè®®
    "è¯·æ±‚",
    "è¯·æ±‚(å¸®åŠ©)",
    "å»ºè®®",
    "é‚€è¯·",
    "æé†’",
    # å›åº”
    "åŒæ„",
    "ä¸åŒæ„",
    "ç†è§£",
    "ä¸ç†è§£",
    "çŸ¥é“äº†",
    "ä¸çŸ¥é“",
    # å…¶ä»–å¸¸ç”¨
    "é—²èŠ",
    "è°ƒä¾ƒ",
    "ç©ç¬‘",
    "å¤¸å¥–",
    "æ‰¹è¯„",
    "æŠ±æ€¨",
    "å®‰æ…°",
    "é¼“åŠ±",
    "å…³å¿ƒ",
    "ç¥ç¦",
}


# ============================================================================
# æ•°æ®åŠ è½½å’Œå¤„ç†
# ============================================================================


def load_telemarketing_data(file_path):
    """åŠ è½½ telemarketing_intent_cn æ•°æ®é›†"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {file_path.name}")

    if not file_path.exists():
        raise FileNotFoundError(f"æ•°æ®é›†ä¸å­˜åœ¨: {file_path}")

    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                data.append(
                    {
                        "text": item["text"],
                        "label": item["label"],
                        "split": item.get("split", "train"),
                    }
                )

    df = pd.DataFrame(data)
    print(f"   åŸå§‹æ•°æ®: {len(df)} æ ·æœ¬, {len(df['label'].unique())} ä¸ªæ„å›¾")

    # åº”ç”¨é»‘åå•
    df = df[~df["label"].isin(BLACKLIST_INTENTS)]
    print(f"   é»‘åå•è¿‡æ»¤å: {len(df)} æ ·æœ¬, {len(df['label'].unique())} ä¸ªæ„å›¾")

    # è¿‡æ»¤æ ·æœ¬æ•°ä¸è¶³çš„æ„å›¾
    intent_counts = Counter(df["label"])
    valid_intents = [
        intent for intent, count in intent_counts.items() if count >= MIN_SAMPLES
    ]
    df = df[df["label"].isin(valid_intents)]
    print(f"   æ ·æœ¬æ•°è¿‡æ»¤å: {len(df)} æ ·æœ¬, {len(df['label'].unique())} ä¸ªæ„å›¾")

    return df


def load_crosswoz_data(file_path):
    """åŠ è½½ CrossWOZ æ•°æ®é›†ï¼ˆæ›´è‡ªç„¶çš„å¯¹è¯ï¼‰"""
    if not file_path.exists():
        print(f"âš ï¸  CrossWOZ æ•°æ®é›†ä¸å­˜åœ¨: {file_path.name}")
        return pd.DataFrame()

    print(f"ğŸ“‚ åŠ è½½ CrossWOZ æ•°æ®: {file_path.name}")

    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                data.append(
                    {
                        "text": item["text"],
                        "label": item["label"],
                        "split": item.get("split", "train"),
                    }
                )

    df = pd.DataFrame(data)
    print(f"   åŸå§‹æ•°æ®: {len(df)} æ ·æœ¬, {len(df['label'].unique())} ä¸ªæ„å›¾")

    # æ˜ å°„ CrossWOZ æ„å›¾åˆ°é€šç”¨æ„å›¾
    intent_mapping = {
        "greet": "æ‹›å‘¼ç”¨è¯­",
        "thank": "ç¤¼è²Œç”¨è¯­",
        "bye": "ç»“æŸç”¨è¯­",
    }

    df["label"] = df["label"].map(intent_mapping)
    df = df.dropna(subset=["label"])

    print(f"   æ˜ å°„å: {len(df)} æ ·æœ¬, {len(df['label'].unique())} ä¸ªæ„å›¾")

    return df


def balance_dataset(df, max_samples_per_intent):
    """å¹³è¡¡æ•°æ®é›†ï¼šé™åˆ¶æ¯ä¸ªæ„å›¾çš„æœ€å¤§æ ·æœ¬æ•°"""
    print(f"\nâš–ï¸  å¹³è¡¡æ•°æ®é›†ï¼ˆæ¯ä¸ªæ„å›¾æœ€å¤š {max_samples_per_intent} æ ·æœ¬ï¼‰")

    balanced_dfs = []
    for intent in df["label"].unique():
        intent_df = df[df["label"] == intent]
        if len(intent_df) > max_samples_per_intent:
            intent_df = intent_df.sample(
                n=max_samples_per_intent, random_state=RANDOM_SEED
            )
        balanced_dfs.append(intent_df)

    balanced_df = pd.concat(balanced_dfs, ignore_index=True)
    print(f"   å¹³è¡¡å: {len(balanced_df)} æ ·æœ¬")

    return balanced_df


def prepare_dataset():
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
    print("=" * 70)
    print("ğŸ“Š å‡†å¤‡æ•°æ®é›†")
    print("=" * 70)

    # åŠ è½½ telemarketing æ•°æ®
    df_telemarketing = load_telemarketing_data(TELEMARKETING_DATA)

    # åŠ è½½ CrossWOZ æ•°æ®ï¼ˆå¯é€‰ï¼‰
    if USE_CROSSWOZ and CROSSWOZ_DATA.exists():
        df_crosswoz = load_crosswoz_data(CROSSWOZ_DATA)
        if not df_crosswoz.empty:
            # åˆå¹¶æ•°æ®é›†
            df = pd.concat([df_telemarketing, df_crosswoz], ignore_index=True)
            print(f"\nâœ… åˆå¹¶æ•°æ®é›†: {len(df)} æ ·æœ¬")
        else:
            df = df_telemarketing
    else:
        df = df_telemarketing

    # å¹³è¡¡æ•°æ®é›†
    df = balance_dataset(df, MAX_SAMPLES_PER_INTENT)

    # ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
    intent_counts = Counter(df["label"])
    print(f"\nğŸ“ˆ æ„å›¾åˆ†å¸ƒï¼ˆå…± {len(intent_counts)} ä¸ªæ„å›¾ï¼‰:")
    for intent, count in intent_counts.most_common(10):
        print(f"   {intent}: {count} æ ·æœ¬")
    if len(intent_counts) > 10:
        print(f"   ... è¿˜æœ‰ {len(intent_counts) - 10} ä¸ªæ„å›¾")

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    unique_intents = sorted(df["label"].unique())
    intent2id = {intent: i for i, intent in enumerate(unique_intents)}
    id2intent = {i: intent for intent, i in intent2id.items()}

    df["label_id"] = df["label"].map(intent2id)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_df, test_df = train_test_split(
        df, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=df["label_id"]
    )

    print(f"\nğŸ“¦ æ•°æ®é›†åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬")

    # è½¬æ¢ä¸º Hugging Face Dataset
    train_dataset = Dataset.from_pandas(
        train_df[["text", "label_id"]].rename(columns={"label_id": "label"})
    )
    test_dataset = Dataset.from_pandas(
        test_df[["text", "label_id"]].rename(columns={"label_id": "label"})
    )

    dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

    return dataset, intent2id, id2intent


# ============================================================================
# æ¨¡å‹è®­ç»ƒ
# ============================================================================


def compute_metrics(eval_pred):
    """è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=-1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")

    return {"accuracy": accuracy, "f1": f1}


def train_model(dataset, intent2id, id2intent):
    """è®­ç»ƒæ¨¡å‹"""
    num_labels = len(intent2id)

    print("\n" + "=" * 70)
    print("ğŸ¤– åŠ è½½æ¨¡å‹")
    print("=" * 70)
    print("æ¨¡å‹: hfl/chinese-bert-wwm-ext")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...\n")

    tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-bert-wwm-ext")
    model = AutoModelForSequenceClassification.from_pretrained(
        "hfl/chinese-bert-wwm-ext", num_labels=num_labels
    )

    model.config.id2label = id2intent
    model.config.label2id = intent2id

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # æ•°æ®é¢„å¤„ç†
    print("\n" + "=" * 70)
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†")
    print("=" * 70)

    def tokenize(examples):
        return tokenizer(
            examples["text"], truncation=True, padding="max_length", max_length=128
        )

    tokenized_dataset = dataset.map(tokenize, batched=True)
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")

    # è®­ç»ƒé…ç½®
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 70)
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"é¢„ä¼°æ—¶é—´: 4-6 å°æ—¶ï¼ˆCPUï¼‰\n")

    training_args = TrainingArguments(
        output_dir=str(OUTPUT_DIR),
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE * 2,
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        logging_steps=50,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        fp16=False,  # Windows CPU ä¸æ”¯æŒ FP16
        report_to="none",  # ä¸ä¸Šä¼ åˆ° wandb
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        compute_metrics=compute_metrics,
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

    return trainer, tokenizer


def evaluate_model(trainer, tokenized_dataset, intent2id):
    """è¯„ä¼°æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹")
    print("=" * 70)

    results = trainer.evaluate()
    print(f"å‡†ç¡®ç‡: {results['eval_accuracy']:.4f}")
    print(f"F1 Score: {results['eval_f1']:.4f}")

    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    predictions = trainer.predict(tokenized_dataset["test"])
    pred_labels = np.argmax(predictions.predictions, axis=-1)
    true_labels = predictions.label_ids

    print("\n" + "=" * 70)
    print("ğŸ“‹ åˆ†ç±»æŠ¥å‘Š")
    print("=" * 70)
    print(
        classification_report(
            true_labels,
            pred_labels,
            target_names=list(intent2id.keys()),
            zero_division=0,
        )
    )


def save_model(trainer, tokenizer, intent2id, id2intent):
    """ä¿å­˜æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
    print("=" * 70)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    trainer.save_model(str(OUTPUT_DIR))
    tokenizer.save_pretrained(str(OUTPUT_DIR))

    # ä¿å­˜æ„å›¾æ˜ å°„
    mapping_path = OUTPUT_DIR / "intent_mapping.json"
    with open(mapping_path, "w", encoding="utf-8") as f:
        json.dump(
            {"intent2id": intent2id, "id2intent": id2intent},
            f,
            indent=2,
            ensure_ascii=False,
        )

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")

    # ä¿å­˜æ„å›¾åˆ—è¡¨ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
    intents_path = OUTPUT_DIR / "intents.txt"
    with open(intents_path, "w", encoding="utf-8") as f:
        f.write("å¾®ä¿¡èŠå¤©æ„å›¾åˆ—è¡¨\n")
        f.write("=" * 50 + "\n\n")
        for i, intent in enumerate(sorted(intent2id.keys()), 1):
            f.write(f"{i}. {intent}\n")

    print(f"âœ… æ„å›¾åˆ—è¡¨å·²ä¿å­˜åˆ°: {intents_path}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


def main():
    print("\n" + "=" * 70)
    print("ğŸ¯ å¾®ä¿¡èŠå¤©æ„å›¾è¯†åˆ«æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print("=" * 70)
    print(f"\né¡¹ç›®è·¯å¾„: {PROJECT_ROOT}")
    print(f"æ•°æ®é›†: {TELEMARKETING_DATA.name}")
    if USE_CROSSWOZ:
        print(f"        + {CROSSWOZ_DATA.name} (å¯é€‰)")
    print(f"æ¨¡å‹ä¿å­˜: {OUTPUT_DIR}\n")

    # æ£€æŸ¥æ•°æ®é›†
    if not TELEMARKETING_DATA.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†ä¸å­˜åœ¨")
        print(f"è·¯å¾„: {TELEMARKETING_DATA}\n")

        # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
        if DATA_DIR.exists():
            print("å¯ç”¨çš„æ•°æ®é›†:")
            for f in sorted(DATA_DIR.glob("*.jsonl"))[:10]:
                print(f"  - {f.name}")
        return

    # å‡†å¤‡æ•°æ®é›†
    dataset, intent2id, id2intent = prepare_dataset()

    # è®­ç»ƒæ¨¡å‹
    trainer, tokenizer = train_model(dataset, intent2id, id2intent)

    # æ•°æ®é¢„å¤„ç†ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    def tokenize(examples):
        return tokenizer(
            examples["text"], truncation=True, padding="max_length", max_length=128
        )

    tokenized_dataset = dataset.map(tokenize, batched=True)

    # è¯„ä¼°æ¨¡å‹
    evaluate_model(trainer, tokenized_dataset, intent2id)

    # ä¿å­˜æ¨¡å‹
    save_model(trainer, tokenizer, intent2id, id2intent)

    # å®Œæˆ
    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"æ¨¡å‹ä½ç½®: {OUTPUT_DIR}")
    print(f"æ„å›¾æ•°é‡: {len(intent2id)}")
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹æ„å›¾åˆ—è¡¨: intents.txt")
    print("  2. è¿è¡Œé¢„æµ‹: python predict_wechat.py")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
